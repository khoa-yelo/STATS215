---
title: "HW2"
author: "Khoa Hoang"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Problem 1

(a)
```{r}
i1 = 0.45
i2 = 0.1
i3 = 0.45
u1 = -3
var1 = 1/3
u2 = 0
var2 = 1/3
u3 = 3
var3 = 1/3
```
```{r}
ptheta1 <- function(){x = rnorm(1, mean = u1, sd = sqrt(var1))
return(x)}
ptheta2 <- function(){x = rnorm(1, mean = u2, sd = sqrt(var2))
return(x)}
ptheta3 <- function(){x = rnorm(1, mean = u3, sd = sqrt(var3))
return(x)}
sample_from = c(ptheta1, ptheta2, ptheta3)
```
(b)
```{r}
ptheta1 <- function(x) {dnorm(x, mean = u1, sd = sqrt(var1))}
ptheta2 <- function(x) {dnorm(x, mean = u2, sd = sqrt(var2))}
ptheta3 <- function(x) {dnorm(x, mean = u3, sd = sqrt(var3))}

next_i <- function(x) {
  #p_theta = (i1*ptheta1(x) + i2*ptheta2(x) + i3*ptheta3(x))
  pi1 = i1*ptheta1(x)#/p_theta
  pi2 = i2*ptheta2(x)#/p_theta
  pi3 = i3*ptheta3(x)#/p_theta
  pis = c(pi1, pi2, pi3)
  return(sample(1:3, 1, prob = pis))
}

```
(c)
```{r}

theta_list = c(0)
i_list = c(2)
# Gibbs sampling
n = 1000
gibbs = function(n){
  for (i in 1:n) {
    indicator = i_list[i]
    theta <- sample_from[indicator][[1]]()
    theta_list = c(theta_list, theta)
    i_list = c(i_list, next_i(theta))
  }
  lists <- list(i_list, theta_list)
  return (lists)
}

set.seed(123)
gibbs_1000 <- gibbs(1000)
hist(gibbs(1000)[[2]])

```
(d) 

```{r}
set.seed(123)
gibbs_40000 <- gibbs(40000)
hist(gibbs(40000)[[2]])

```

(e) Try trace plots for the Î¸ and plots of the autocorrelation functions for parts (c) and (d).
Comment on what these MCMC diagnostics tell you.

```{r}
# plot side by side
par(mfrow=c(2,2))
plot(gibbs_1000[[2]], type = "l")
plot(gibbs_40000[[2]], type = "l")
```

```{r}
par(mfrow=c(2,2))
acf(gibbs_1000[[2]])
acf(gibbs_40000[[2]])
```
Theta are highly correlated to each other. This shows that one may need to run MCMC for a long time and thin the samples to get independent samples.


Problem 2
(a)
```{r}
seq1 = "ATGCGTA"
seq2 = "CGTGCAT"
seq3 = "AGTACAA"
seq4 = "TCGATGA"
seq5 = "GGGCTAC"
seqs = c(seq1, seq2, seq3, seq4, seq5)


```

(b)
```{r}
seq_length = nchar(seqs[1])
window_size = 3
initial_positions = c(1, 1, 1, 1, 1)

one_hot_encoding <- function(seq){
  seq_length = nchar(seq)
  one_hot = matrix(0, nrow = 4, ncol = seq_length)
  for (i in 1:seq_length){
    base = substr(seq, i, i)
    if (base == "A"){
      one_hot[1, i] = 1
    } else if (base == "C"){
      one_hot[2, i] = 1
    } else if (base == "G"){
      one_hot[3, i] = 1
    } else if (base == "T"){
      one_hot[4, i] = 1
    }
  }
  return(one_hot)
}

calculate_positional_matrix <- function(seqs, window_size, start_pos_list, drop_index){
  positional_matrix = matrix(0, nrow = 4, ncol = window_size)
  i = 0
  seqs = seqs[-drop_index]
  start_pos_list = start_pos_list[-drop_index]
  for (seq in seqs){
    i = i + 1
    start_pos = start_pos_list[i]
    sub_seq = substr(seq, start_pos, start_pos + window_size - 1)
    one_hot = one_hot_encoding(sub_seq)
    positional_matrix = positional_matrix + one_hot
  }
  positional_matrix = positional_matrix / length(seqs)
  return(positional_matrix)
}

cal_llh <- function(positional_matrix, seq){
  seq_length = nchar(seq)
  llh_list = c()
  for (i in 1:(seq_length - window_size + 1)){
    sub_seq = substr(seq, i, i + window_size - 1)
    one_hot = one_hot_encoding(sub_seq)
    prob = 1
    llh = sum(one_hot * positional_matrix)
    llh_list = c(llh_list, llh)
  }
  return(llh_list)
}

calculate_entropy <- function(pwm) {
  entropy <- 0
  for (col in t(pwm)) {
    col_entropy <- 0
    for (p in col) {
      if (p != 0) {
        col_entropy <- col_entropy - p * log2(p)
      }
    }
    entropy <- entropy + col_entropy
  }
  return(entropy)
}

```

```{r}

positions = initial_positions
drop_seq_index = 1
n = 10000
# all_positions is a matrix of n by 5 where n is the number of iterations
all_positions = matrix(0, nrow = n, ncol = 5)
entropy_list = list()
for (i in 1:n){
  pos_mat <- calculate_positional_matrix(seqs, window_size, positions, drop_seq_index)
  llhs = cal_llh(pos_mat, seqs[drop_seq_index])
  # choose a position with probability proportional to the likelihood
  positions[drop_seq_index] = sample(1:(seq_length - window_size + 1), 1, prob = llhs)
  drop_seq_index = (drop_seq_index %% 5) + 1
  #update all_positions
  all_positions[i,] = positions  
  pos_mat <- calculate_positional_matrix(seqs, window_size, positions, 1000000)
  entropy = calculate_entropy(pos_mat)
  entropy_list = append(entropy_list, entropy)
}
```

(d)
```{r}
#get index of lowest entropy
lowest_entropy_index = which.min(unlist(entropy_list))
#get position of the lowest entropy
lowest_entropy_position = all_positions[lowest_entropy_index,]
lowest_entropy_position
#generate sequence logo of the lowest entropy position
pos_mat <- calculate_positional_matrix(seqs, window_size, lowest_entropy_position, 1000000)
#generate sequence logo image
library(seqLogo)
seqLogo(pos_mat)

```

Problem 3: Wright Fisher Model

(a)
```{r}

drift <- function(N, n, p){
  minor_all = c()
  while (n > 0 & n < 2*N){
    n <- rbinom(1, 2*N, p)
    p <- n/(2*N)
    minor_all = c(minor_all, n)
  }
  return(minor_all)
}
```

(b)
```{r}
N = 1000
n = 400
p = n/(2*N)
# run drift 1000 times
res <- replicate(1000, drift(N, n, p))
max_length <- max(sapply(res, length))

# pad the list with last value
res_pad <- lapply(res, function(x) c(x, rep(x[length(x)], max_length - length(x))))
res_pad_df <- data.frame(res_pad)
colnames(res_pad_df) <- 1:1000

for (i in 1:10){
  res_pad_df[,i] <- as.numeric(res_pad_df[,i])
  # plot all lines in the same graph
  plot(res_pad_df[,i], type = 'l', col = i, lty = 1, xlab = 'X Axis Label', ylab = 'Y Axis Label', main = 'Title')
  
}
```

(c)
```{r}
# get final values of res
final_values <- sapply(res, tail, 1)
frequency <- table(final_values)
probability <- frequency/sum(frequency)
probability


```
prob = 0.197 

(d)
```{r}
# length of each list in res
lengths <- sapply(res, length)
hist(lengths, main = "Histogram of Lengths", xlab = "Lengths", ylab = "Frequency")
mean(lengths)

# lengths of each list in res with final value of 2000
length_minors <- lengths[final_values == 2000]
hist(length_minors, main = "Histogram of Lengths with Final Value of 2000", xlab = "Lengths", ylab = "Frequency")
mean(length_minors)

```
2054.534 generation for either allele to be fix
3480.548 generation for minor allele to be fix

Problem 4

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("dada2")
```
(a)

```{r}
base_dir = "C:\\Users\\khoah\\PhD_Documents\\SusanHolmes_Book_works\\data"
miseq_path = file.path(base_dir, "MiSeq_SOP")
filt_path = file.path(miseq_path, "filtered")
fnFs = sort(list.files(miseq_path, pattern="_R1_001.fastq"))
fnRs = sort(list.files(miseq_path, pattern="_R2_001.fastq"))
sampleNames = sapply(strsplit(fnFs, "_"), `[`, 1)
if (!file_test("-d", filt_path)) dir.create(filt_path)
filtFs = file.path(filt_path, paste0(sampleNames, "_F_filt.fastq.gz"))
filtRs = file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
fnFs = file.path(miseq_path, fnFs)
fnRs = file.path(miseq_path, fnRs)
print(length(fnFs))
```
(b)
```{r}
library(dada2)
plotQualityProfile(fnFs[1:2])
plotQualityProfile(fnRs[1:2])

```
(c)
```{r}

out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
                  maxN=0, maxEE=c(2,5), truncQ=2, rm.phix=TRUE,
                  compress=TRUE, multithread=F, trimLeft = c(10, 10))
```

(d)

```{r}
plotQualityProfile(filtFs[1:2])
plotQualityProfile(filtRs[1:2])
derepFs = derepFastq(filtFs, verbose = FALSE)
derepRs = derepFastq(filtRs, verbose = FALSE)
names(derepFs) = sampleNames
names(derepRs) = sampleNames
ddF = dada(derepFs, err = NULL, selfConsist = TRUE)
ddR = dada(derepRs, err = NULL, selfConsist = TRUE)
plotErrors(ddF)
plotErrors(ddR)


dadaFs <- dada(filtFs, err=ddF[[1]]$err_out, multithread=FALSE)
dadaRs <- dada(filtRs, err=ddR[[1]]$err_out, multithread=FALSE)
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
seqtab <- makeSequenceTable(mergers)

```

